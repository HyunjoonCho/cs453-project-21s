{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deepxplore_MNIST.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNRtkQtUCIbbg5jsMhL2wAi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77RONfURL9Gm","executionInfo":{"status":"ok","timestamp":1622209630742,"user_tz":-540,"elapsed":3405,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}},"outputId":"6be6c8c3-05d0-463f-aa06-ac93c297d749"},"source":["!pip install np-utils\n","# from kera.utils import to_categorical does not work! "],"execution_count":30,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: np-utils in /usr/local/lib/python3.7/dist-packages (0.5.12.1)\n","Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np-utils) (1.19.5)\n","Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from np-utils) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aaQ7v-eQGw30","executionInfo":{"status":"ok","timestamp":1622209634190,"user_tz":-540,"elapsed":393,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}}},"source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1hFBk0k_RmP","executionInfo":{"status":"ok","timestamp":1622209637116,"user_tz":-540,"elapsed":564,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}}},"source":["'''\n","LeNet-1\n","'''\n","\n","# usage: python MNISTModel1.py - train the model\n","\n","from __future__ import print_function\n","\n","from keras.datasets import mnist\n","from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n","from keras.models import Model\n","from keras.utils.np_utils import to_categorical\n","\n","def Model1(input_tensor=None, train=False):\n","    nb_classes = 10\n","    # convolution kernel size\n","    kernel_size = (5, 5)\n","\n","    if train:\n","        batch_size = 256\n","        nb_epoch = 10\n","\n","        # input image dimensions\n","        img_rows, img_cols = 28, 28\n","\n","        # the data, shuffled and split between train and test sets\n","        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","        print(x_train.shape)\n","        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","        input_shape = (img_rows, img_cols, 1)\n","\n","        x_train = x_train.astype('float32')\n","        x_test = x_test.astype('float32')\n","        x_train /= 255\n","        x_test /= 255\n","\n","        # convert class vectors to binary class matrices\n","        y_train = to_categorical(y_train, nb_classes)\n","        y_test = to_categorical(y_test, nb_classes)\n","\n","        input_tensor = Input(shape=input_shape)\n","    elif input_tensor is None:\n","        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n","        exit()\n","\n","    # block1\n","    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n","    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n","\n","    # block2\n","    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n","    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n","\n","    x = Flatten(name='flatten')(x)\n","    x = Dense(nb_classes, name='before_softmax')(x)\n","    x = Activation('softmax', name='predictions')(x)\n","\n","    model = Model(input_tensor, x)\n","\n","    if train:\n","        # compiling\n","        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n","\n","        # trainig\n","        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n","        # save model\n","        model.save_weights('./Model1.h5')\n","        score = model.evaluate(x_test, y_test, verbose=0)\n","        print('\\n')\n","        print('Overall Test score:', score[0])\n","        print('Overall Test accuracy:', score[1])\n","    else:\n","        model.load_weights('./Model1.h5')\n","        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n","\n","    return model"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQaLFAXqGZph","executionInfo":{"status":"ok","timestamp":1622209641086,"user_tz":-540,"elapsed":563,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}}},"source":["'''\n","LeNet-4\n","'''\n","\n","# usage: python MNISTModel2.py - train the model\n","\n","from __future__ import print_function\n","\n","from keras.datasets import mnist\n","from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n","from keras.models import Model\n","from keras.utils.np_utils import to_categorical\n","\n","def Model2(input_tensor=None, train=False):\n","    nb_classes = 10\n","    # convolution kernel size\n","    kernel_size = (5, 5)\n","\n","    if train:\n","        batch_size = 256\n","        nb_epoch = 10\n","\n","        # input image dimensions\n","        img_rows, img_cols = 28, 28\n","\n","        # the data, shuffled and split between train and test sets\n","        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","        input_shape = (img_rows, img_cols, 1)\n","\n","        x_train = x_train.astype('float32')\n","        x_test = x_test.astype('float32')\n","        x_train /= 255\n","        x_test /= 255\n","\n","        # convert class vectors to binary class matrices\n","        y_train = to_categorical(y_train, nb_classes)\n","        y_test = to_categorical(y_test, nb_classes)\n","\n","        input_tensor = Input(shape=input_shape)\n","    elif input_tensor is None:\n","        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n","        exit()\n","\n","    # block1\n","    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n","    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n","\n","    # block2\n","    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n","    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n","\n","    x = Flatten(name='flatten')(x)\n","    x = Dense(84, activation='relu', name='fc1')(x)\n","    x = Dense(nb_classes, name='before_softmax')(x)\n","    x = Activation('softmax', name='predictions')(x)\n","\n","    model = Model(input_tensor, x)\n","\n","    if train:\n","        # compiling\n","        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n","\n","        # trainig\n","        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n","        # save model\n","        model.save_weights('./Model2.h5')\n","        score = model.evaluate(x_test, y_test, verbose=0)\n","        print('\\n')\n","        print('Overall Test score:', score[0])\n","        print('Overall Test accuracy:', score[1])\n","    else:\n","        model.load_weights('./Model2.h5')\n","        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n","\n","    return model"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSeML7vsGZmj","executionInfo":{"status":"ok","timestamp":1622209644877,"user_tz":-540,"elapsed":467,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}}},"source":["'''\n","LeNet-5\n","'''\n","\n","# usage: python MNISTModel3.py - train the model\n","\n","from __future__ import print_function\n","\n","from keras.datasets import mnist\n","from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n","from keras.models import Model\n","from keras.utils.np_utils import to_categorical\n","\n","\n","def Model3(input_tensor=None, train=False):\n","    nb_classes = 10\n","    # convolution kernel size\n","    kernel_size = (5, 5)\n","\n","    if train:\n","        batch_size = 256\n","        nb_epoch = 10\n","\n","        # input image dimensions\n","        img_rows, img_cols = 28, 28\n","\n","        # the data, shuffled and split between train and test sets\n","        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","        input_shape = (img_rows, img_cols, 1)\n","\n","        x_train = x_train.astype('float32')\n","        x_test = x_test.astype('float32')\n","        x_train /= 255\n","        x_test /= 255\n","\n","        # convert class vectors to binary class matrices\n","        y_train = to_categorical(y_train, nb_classes)\n","        y_test = to_categorical(y_test, nb_classes)\n","\n","        input_tensor = Input(shape=input_shape)\n","    elif input_tensor is None:\n","        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n","        exit()\n","\n","    # block1\n","    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n","    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n","\n","    # block2\n","    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n","    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n","\n","    x = Flatten(name='flatten')(x)\n","    x = Dense(120, activation='relu', name='fc1')(x)\n","    x = Dense(84, activation='relu', name='fc2')(x)\n","    x = Dense(nb_classes, name='before_softmax')(x)\n","    x = Activation('softmax', name='predictions')(x)\n","\n","    model = Model(input_tensor, x)\n","\n","    if train:\n","        # compiling\n","        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n","\n","        # trainig\n","        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n","        # save model\n","        model.save_weights('./Model3.h5')\n","        score = model.evaluate(x_test, y_test, verbose=0)\n","        print('\\n')\n","        print('Overall Test score:', score[0])\n","        print('Overall Test accuracy:', score[1])\n","    else:\n","        model.load_weights('./Model3.h5')\n","        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n","\n","    return model"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"VF11KG8DGzE0","executionInfo":{"status":"ok","timestamp":1622211142438,"user_tz":-540,"elapsed":647,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}}},"source":["import random\n","from collections import defaultdict\n","\n","import numpy as np\n","from keras import backend as K\n","from keras.models import Model\n","\n","\n","# util function to convert a tensor into a valid image\n","def deprocess_image(x):\n","    x *= 255\n","    x = np.clip(x, 0, 255).astype('uint8')\n","    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n","\n","\n","def normalize(x):\n","    # utility function to normalize a tensor by its L2 norm\n","    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n","\n","\n","def constraint_occl(gradients, start_point, rect_shape):\n","    new_grads = np.zeros_like(gradients)\n","    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n","    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n","                                                     start_point[1]:start_point[1] + rect_shape[1]]\n","    return new_grads\n","\n","\n","def constraint_light(gradients):\n","    new_grads = np.ones_like(gradients)\n","    grad_mean = np.mean(gradients)\n","    return grad_mean * new_grads\n","\n","\n","def constraint_black(gradients, rect_shape=(6, 6)):\n","    start_point = (\n","        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n","    new_grads = np.zeros_like(gradients)\n","    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n","    if np.mean(patch) < 0:\n","        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n","        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n","    return new_grads\n","\n","\n","def init_coverage_tables(model1, model2, model3):\n","    model_layer_dict1 = defaultdict(bool)\n","    model_layer_dict2 = defaultdict(bool)\n","    model_layer_dict3 = defaultdict(bool)\n","    init_dict(model1, model_layer_dict1)\n","    init_dict(model2, model_layer_dict2)\n","    init_dict(model3, model_layer_dict3)\n","    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n","\n","\n","def init_dict(model, model_layer_dict):\n","    for layer in model.layers:\n","        if 'flatten' in layer.name or 'input' in layer.name:\n","            continue\n","        for index in range(layer.output_shape[-1]):\n","            model_layer_dict[(layer.name, index)] = False\n","\n","\n","def neuron_to_cover(model_layer_dict):\n","    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n","    if not_covered:\n","        layer_name, index = random.choice(not_covered)\n","    else:\n","        layer_name, index = random.choice(model_layer_dict.keys())\n","    return layer_name, index\n","\n","\n","def neuron_covered(model_layer_dict):\n","    covered_neurons = len([v for v in model_layer_dict.values() if v])\n","    total_neurons = len(model_layer_dict)\n","    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n","\n","\n","def update_coverage(input_data, model, model_layer_dict, threshold=0):\n","    layer_names = [layer.name for layer in model.layers if\n","                   'flatten' not in layer.name and 'input' not in layer.name]\n","\n","    intermediate_layer_model = Model(inputs=model.input,\n","                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n","    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n","\n","    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n","        scaled = scale(intermediate_layer_output[0])\n","        for num_neuron in range(scaled.shape[-1]):\n","            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n","                model_layer_dict[(layer_names[i], num_neuron)] = True\n","\n","\n","def full_coverage(model_layer_dict):\n","    if False in model_layer_dict.values():\n","        return False\n","    return True\n","\n","\n","def scale(intermediate_layer_output, rmax=1, rmin=0):\n","    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n","        intermediate_layer_output.max() - intermediate_layer_output.min())\n","    X_scaled = X_std * (rmax - rmin) + rmin\n","    return X_scaled\n","\n","\n","def fired(model, layer_name, index, input_data, threshold=0):\n","    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n","    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n","    scaled = scale(intermediate_layer_output)\n","    if np.mean(scaled[..., index]) > threshold:\n","        return True\n","    return False\n","\n","\n","def diverged(predictions1, predictions2, predictions3, target):\n","    #     if predictions2 == predictions3 == target and predictions1 != target:\n","    if not predictions1 == predictions2 == predictions3:\n","        return True\n","    return False"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"7UA9r1RXG46F","executionInfo":{"status":"ok","timestamp":1622211278837,"user_tz":-540,"elapsed":1176,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}}},"source":["from __future__ import print_function\n","\n","import argparse\n","\n","from keras.datasets import mnist\n","from keras.layers import Input\n","from imageio import imwrite\n","\n","import os\n","\n","def gen_diff(arguments_list):\n","    # read the parameter\n","    # argument parsing\n","    parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n","    parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n","    parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n","    parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n","    parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n","    parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n","    parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n","    parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n","    parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n","                        choices=[0, 1, 2], default=0, type=int)\n","    parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n","    parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)\n","\n","    args = parser.parse_args(arguments_list)\n","\n","    # input image dimensions\n","    img_rows, img_cols = 28, 28\n","    # the data, shuffled and split between train and test sets\n","    (_, _), (x_test, _) = mnist.load_data()\n","\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","    x_test = x_test.astype('float32')\n","    x_test /= 255\n","\n","    # define input tensor as a placeholder\n","    input_tensor = Input(shape=input_shape)\n","\n","    # load multiple models sharing same input tensor\n","    model1 = Model1(input_tensor=input_tensor)\n","    model2 = Model2(input_tensor=input_tensor)\n","    model3 = Model3(input_tensor=input_tensor)\n","\n","    # init coverage table\n","    model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)\n","\n","    # ==============================================================================================\n","    # start gen inputs\n","    for _ in range(args.seeds):\n","        gen_img = np.expand_dims(random.choice(x_test), axis=0)\n","        orig_img = gen_img.copy()\n","        # first check if input already induces differences\n","        label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(\n","            model3.predict(gen_img)[0])\n","\n","        if not label1 == label2 == label3:\n","            print(bcolors.OKGREEN + 'input already causes different outputs: {}, {}, {}'.format(label1, label2,\n","                                                                                                label3) + bcolors.ENDC)\n","\n","            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n","            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n","            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n","\n","            print(bcolors.OKGREEN + 'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n","                  % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n","                    neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n","                    neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n","            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n","                          neuron_covered(model_layer_dict3)[0]) / float(\n","                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n","                neuron_covered(model_layer_dict3)[\n","                    1])\n","            print(bcolors.OKGREEN + 'averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n","\n","            gen_img_deprocessed = deprocess_image(gen_img)\n","\n","            # save the result to disk\n","\n","            if not os.path.isdir('./generated_inputs'):\n","                os.mkdir('./generated_inputs/')\n","\n","            imwrite('./generated_inputs/' + 'already_differ_' + str(label1) + '_' + str(\n","                label2) + '_' + str(label3) + '.png', gen_img_deprocessed)\n","            continue\n","\n","        # if all label agrees\n","        orig_label = label1\n","        layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n","        layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n","        layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n","\n","        # construct joint loss function\n","        if args.target_model == 0:\n","            loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n","            loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n","            loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n","        elif args.target_model == 1:\n","            loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n","            loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n","            loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n","        elif args.target_model == 2:\n","            loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n","            loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n","            loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n","        loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n","        loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n","        loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n","        layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n","\n","        # for adversarial image generation\n","        final_loss = K.mean(layer_output)\n","\n","        # we compute the gradient of the input picture wrt this loss\n","        grads = normalize(K.gradients(final_loss, input_tensor)[0])\n","\n","        # this function returns the loss and grads given the input picture\n","        iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n","\n","        # we run gradient ascent for 20 steps\n","        for iters in range(args.grad_iterations):\n","            loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate(\n","                [gen_img])\n","            if args.transformation == 'light':\n","                grads_value = constraint_light(grads_value)  # constraint the gradients value\n","            elif args.transformation == 'occl':\n","                grads_value = constraint_occl(grads_value, args.start_point,\n","                                              args.occlusion_size)  # constraint the gradients value\n","            elif args.transformation == 'blackout':\n","                grads_value = constraint_black(grads_value)  # constraint the gradients value\n","\n","            gen_img += grads_value * args.step\n","            predictions1 = np.argmax(model1.predict(gen_img)[0])\n","            predictions2 = np.argmax(model2.predict(gen_img)[0])\n","            predictions3 = np.argmax(model3.predict(gen_img)[0])\n","\n","            if not predictions1 == predictions2 == predictions3:\n","                update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n","                update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n","                update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n","\n","                print(bcolors.OKGREEN + 'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n","                      % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n","                        neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n","                        neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n","                averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n","                              neuron_covered(model_layer_dict3)[0]) / float(\n","                    neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n","                    neuron_covered(model_layer_dict3)[\n","                        1])\n","                print(bcolors.OKGREEN + 'averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n","\n","                gen_img_deprocessed = deprocess_image(gen_img)\n","                orig_img_deprocessed = deprocess_image(orig_img)\n","\n","                # save the result to disk\n","                imwrite('./generated_inputs/' + args.transformation + '_' + str(predictions1) + '_' + str(\n","                    predictions2) + '_' + str(predictions3) + '.png',\n","                      gen_img_deprocessed)\n","                imwrite('./generated_inputs/' + args.transformation + '_' + str(predictions1) + '_' + str(\n","                    predictions2) + '_' + str(predictions3) + '_orig.png',\n","                      orig_img_deprocessed)\n","                break"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4V_F1yu1Glyw","executionInfo":{"status":"ok","timestamp":1622210895864,"user_tz":-540,"elapsed":1234470,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}},"outputId":"26a7ffc0-6319-49fc-c83e-a56d22280d5c"},"source":["Model1(train=True)\n","Model2(train=True)\n","Model3(train=True)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["(60000, 28, 28)\n","Epoch 1/10\n","235/235 [==============================] - 36s 147ms/step - loss: 2.3294 - accuracy: 0.1025 - val_loss: 2.3243 - val_accuracy: 0.1017\n","Epoch 2/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.3216 - accuracy: 0.1060 - val_loss: 2.3171 - val_accuracy: 0.1032\n","Epoch 3/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.3158 - accuracy: 0.1064 - val_loss: 2.3099 - val_accuracy: 0.1060\n","Epoch 4/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.3082 - accuracy: 0.1107 - val_loss: 2.3027 - val_accuracy: 0.1089\n","Epoch 5/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.3015 - accuracy: 0.1152 - val_loss: 2.2956 - val_accuracy: 0.1141\n","Epoch 6/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.2934 - accuracy: 0.1215 - val_loss: 2.2886 - val_accuracy: 0.1213\n","Epoch 7/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.2864 - accuracy: 0.1295 - val_loss: 2.2816 - val_accuracy: 0.1309\n","Epoch 8/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.2791 - accuracy: 0.1393 - val_loss: 2.2746 - val_accuracy: 0.1482\n","Epoch 9/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.2737 - accuracy: 0.1525 - val_loss: 2.2676 - val_accuracy: 0.1659\n","Epoch 10/10\n","235/235 [==============================] - 34s 144ms/step - loss: 2.2658 - accuracy: 0.1691 - val_loss: 2.2606 - val_accuracy: 0.1839\n","\n","\n","Overall Test score: 2.260607957839966\n","Overall Test accuracy: 0.18389999866485596\n","Epoch 1/10\n","235/235 [==============================] - 40s 167ms/step - loss: 2.3170 - accuracy: 0.1011 - val_loss: 2.3075 - val_accuracy: 0.1084\n","Epoch 2/10\n","235/235 [==============================] - 39s 165ms/step - loss: 2.3025 - accuracy: 0.1129 - val_loss: 2.2940 - val_accuracy: 0.1246\n","Epoch 3/10\n","235/235 [==============================] - 39s 165ms/step - loss: 2.2895 - accuracy: 0.1297 - val_loss: 2.2802 - val_accuracy: 0.1479\n","Epoch 4/10\n","235/235 [==============================] - 39s 165ms/step - loss: 2.2774 - accuracy: 0.1492 - val_loss: 2.2662 - val_accuracy: 0.1689\n","Epoch 5/10\n","235/235 [==============================] - 39s 167ms/step - loss: 2.2631 - accuracy: 0.1721 - val_loss: 2.2519 - val_accuracy: 0.1935\n","Epoch 6/10\n","235/235 [==============================] - 39s 166ms/step - loss: 2.2492 - accuracy: 0.1973 - val_loss: 2.2372 - val_accuracy: 0.2264\n","Epoch 7/10\n","235/235 [==============================] - 39s 165ms/step - loss: 2.2339 - accuracy: 0.2355 - val_loss: 2.2219 - val_accuracy: 0.2688\n","Epoch 8/10\n","235/235 [==============================] - 39s 165ms/step - loss: 2.2193 - accuracy: 0.2757 - val_loss: 2.2057 - val_accuracy: 0.3051\n","Epoch 9/10\n","235/235 [==============================] - 39s 166ms/step - loss: 2.2024 - accuracy: 0.3132 - val_loss: 2.1882 - val_accuracy: 0.3330\n","Epoch 10/10\n","235/235 [==============================] - 39s 166ms/step - loss: 2.1849 - accuracy: 0.3418 - val_loss: 2.1694 - val_accuracy: 0.3594\n","\n","\n","Overall Test score: 2.1693663597106934\n","Overall Test accuracy: 0.3594000041484833\n","Epoch 1/10\n","235/235 [==============================] - 41s 169ms/step - loss: 2.3004 - accuracy: 0.1079 - val_loss: 2.2916 - val_accuracy: 0.1223\n","Epoch 2/10\n","235/235 [==============================] - 39s 167ms/step - loss: 2.2894 - accuracy: 0.1180 - val_loss: 2.2801 - val_accuracy: 0.1360\n","Epoch 3/10\n","235/235 [==============================] - 39s 167ms/step - loss: 2.2778 - accuracy: 0.1384 - val_loss: 2.2683 - val_accuracy: 0.1668\n","Epoch 4/10\n","235/235 [==============================] - 39s 168ms/step - loss: 2.2663 - accuracy: 0.1708 - val_loss: 2.2561 - val_accuracy: 0.2172\n","Epoch 5/10\n","235/235 [==============================] - 39s 168ms/step - loss: 2.2539 - accuracy: 0.2209 - val_loss: 2.2433 - val_accuracy: 0.2677\n","Epoch 6/10\n","235/235 [==============================] - 40s 169ms/step - loss: 2.2416 - accuracy: 0.2660 - val_loss: 2.2296 - val_accuracy: 0.3084\n","Epoch 7/10\n","235/235 [==============================] - 40s 169ms/step - loss: 2.2272 - accuracy: 0.3026 - val_loss: 2.2146 - val_accuracy: 0.3309\n","Epoch 8/10\n","235/235 [==============================] - 40s 168ms/step - loss: 2.2122 - accuracy: 0.3285 - val_loss: 2.1979 - val_accuracy: 0.3501\n","Epoch 9/10\n","235/235 [==============================] - 40s 169ms/step - loss: 2.1956 - accuracy: 0.3462 - val_loss: 2.1793 - val_accuracy: 0.3644\n","Epoch 10/10\n","235/235 [==============================] - 40s 169ms/step - loss: 2.1766 - accuracy: 0.3624 - val_loss: 2.1584 - val_accuracy: 0.3820\n","\n","\n","Overall Test score: 2.158369302749634\n","Overall Test accuracy: 0.38199999928474426\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.engine.functional.Functional at 0x7f0e64e0a490>"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0KL65j8PG9LL","executionInfo":{"status":"ok","timestamp":1622211289775,"user_tz":-540,"elapsed":6321,"user":{"displayName":"Hyunjoon Cho","photoUrl":"","userId":"13128687037992418245"}},"outputId":"1fbf4ed7-0d18-4584-a4cc-3bbbf15b7443"},"source":["# transformation, weight_diff, weight_nc, step, seeds, grad_iterations, threshold\n","gen_diff(['light', '.3', '.3', '.3', '5', '3', '0.7'])"],"execution_count":48,"outputs":[{"output_type":"stream","text":["\u001b[94mModel1 loaded\u001b[0m\n","\u001b[94mModel2 loaded\u001b[0m\n","\u001b[94mModel3 loaded\u001b[0m\n","\u001b[92minput already causes different outputs: 3, 6, 6\u001b[0m\n","\u001b[92mcovered neurons percentage 52 neurons 0.058, 148 neurons 0.081, 268 neurons 0.049\u001b[0m\n","\u001b[92maveraged covered neurons 0.060\u001b[0m\n","\u001b[92minput already causes different outputs: 9, 6, 8\u001b[0m\n","\u001b[92mcovered neurons percentage 52 neurons 0.096, 148 neurons 0.095, 268 neurons 0.090\u001b[0m\n","\u001b[92maveraged covered neurons 0.092\u001b[0m\n","\u001b[92minput already causes different outputs: 8, 7, 2\u001b[0m\n","\u001b[92mcovered neurons percentage 52 neurons 0.135, 148 neurons 0.149, 268 neurons 0.108\u001b[0m\n","\u001b[92maveraged covered neurons 0.124\u001b[0m\n","\u001b[92minput already causes different outputs: 3, 7, 2\u001b[0m\n","\u001b[92mcovered neurons percentage 52 neurons 0.135, 148 neurons 0.196, 268 neurons 0.134\u001b[0m\n","\u001b[92maveraged covered neurons 0.154\u001b[0m\n","\u001b[92minput already causes different outputs: 3, 8, 2\u001b[0m\n","\u001b[92mcovered neurons percentage 52 neurons 0.135, 148 neurons 0.209, 268 neurons 0.153\u001b[0m\n","\u001b[92maveraged covered neurons 0.169\u001b[0m\n"],"name":"stdout"}]}]}